{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Step Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_h error:  1.85037348267e-09\n"
     ]
    }
   ],
   "source": [
    "from code_base.rnn_layers import rnn_step_forward\n",
    "from code_base.layer_utils import rel_error\n",
    "import numpy as np\n",
    "\n",
    "N, D, H = 3, 10, 4\n",
    "x = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)\n",
    "prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)\n",
    "Wh = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)\n",
    "b = np.linspace(-0.2, 0.4, num=H)\n",
    "next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)\n",
    "expected_next_h = np.asarray([\n",
    "[-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n",
    "[ 0.66854692, 0.79562378, 0.87755553, 0.92795967],\n",
    "[ 0.97934501, 0.99144213, 0.99646691, 0.99854353]])\n",
    "print('next_h error: ', rel_error(expected_next_h, next_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_base.rnn_layers import rnn_step_forward\n",
    "import numpy as np\n",
    "\n",
    "x_shape = (3, 874)\n",
    "Wx_shape = (874, 128)\n",
    "h_shape = (3, 128)\n",
    "Wh_shape = (128, 128)\n",
    "b_shape = (128,)\n",
    "x = np.loadtxt('./input_files/x.csv', delimiter=',')\n",
    "x = x.reshape(x_shape)\n",
    "Wx = np.loadtxt('./input_files/Wx.csv', delimiter=',')\n",
    "Wx = Wx.reshape(Wx_shape)\n",
    "prev_h = np.loadtxt('./input_files/prev_h.csv', delimiter=',')\n",
    "prev_h = prev_h.reshape(h_shape)\n",
    "Wh = np.loadtxt('./input_files/Wh.csv', delimiter=',')\n",
    "Wh = Wh.reshape(Wh_shape)\n",
    "b = np.loadtxt('./input_files/b.csv', delimiter=',')\n",
    "out, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)\n",
    "np.savetxt('./output_files/rnn_step_forward_out.csv', out.ravel(), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Step Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  3.75175611382e-11\n",
      "dprev_h error:  1.4932453546e-10\n",
      "dWx error:  5.31796793219e-11\n",
      "dWh error:  4.63911517973e-11\n",
      "db error:  1.35737554145e-11\n"
     ]
    }
   ],
   "source": [
    "from code_base.rnn_layers import rnn_step_forward, rnn_step_backward\n",
    "from code_base.gradient_check import *\n",
    "from code_base.layer_utils import rel_error\n",
    "import numpy as np\n",
    "\n",
    "N, D, H = 4, 5, 6\n",
    "x = np.random.randn(N, D)\n",
    "h = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, H)\n",
    "Wh = np.random.randn(H, H)\n",
    "b = np.random.randn(H)\n",
    "out, cache = rnn_step_forward(x, h, Wx, Wh, b)\n",
    "dnext_h = np.random.randn(*out.shape)\n",
    "fx = lambda x: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fh = lambda prev_h: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fb = lambda b: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dnext_h)\n",
    "dprev_h_num = eval_numerical_gradient_array(fh, h, dnext_h)\n",
    "dWx_num = eval_numerical_gradient_array(fWx, Wx, dnext_h)\n",
    "dWh_num = eval_numerical_gradient_array(fWh, Wh, dnext_h)\n",
    "db_num = eval_numerical_gradient_array(fb, b, dnext_h)\n",
    "dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dprev_h error: ', rel_error(dprev_h_num, dprev_h))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_base.rnn_layers import rnn_step_forward, rnn_step_backward\n",
    "import numpy as np\n",
    "\n",
    "x_shape = (3, 874)\n",
    "Wx_shape = (874, 128)\n",
    "h_shape = (3, 128)\n",
    "Wh_shape = (128, 128)\n",
    "b_shape = (128,)\n",
    "x = np.loadtxt('./input_files/x.csv', delimiter=',')\n",
    "x = x.reshape(x_shape)\n",
    "Wx = np.loadtxt('./input_files/Wx.csv', delimiter=',')\n",
    "Wx = Wx.reshape(Wx_shape)\n",
    "prev_h = np.loadtxt('./input_files/prev_h.csv', delimiter=',')\n",
    "prev_h = prev_h.reshape(h_shape)\n",
    "Wh = np.loadtxt('./input_files/Wh.csv', delimiter=',')\n",
    "Wh = Wh.reshape(Wh_shape)\n",
    "b = np.loadtxt('./input_files/b.csv', delimiter=',')\n",
    "out, cache = rnn_step_forward(x, prev_h, Wx, Wh, b)\n",
    "dhout = np.loadtxt('./input_files/dho.csv', delimiter=',')\n",
    "dx, dh, dWx, dWh, db = rnn_step_backward(dhout, cache)\n",
    "np.savetxt('./output_files/rnn_step_backward_out_dx.csv', dx.ravel(), delimiter=',')\n",
    "np.savetxt('./output_files/rnn_step_backward_out_dh.csv', dh.ravel(), delimiter=',')\n",
    "np.savetxt('./output_files/rnn_step_backward_out_dwx.csv', dWx.ravel(), delimiter=',')\n",
    "np.savetxt('./output_files/rnn_step_backward_out_dwh.csv', dWh.ravel(), delimiter=',')\n",
    "np.savetxt('./output_files/rnn_step_backward_out_db.csv', db.ravel(), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h error:  1.04522949386e-08\n"
     ]
    }
   ],
   "source": [
    "from code_base.rnn_layers import rnn_forward\n",
    "from code_base.layer_utils import *\n",
    "import numpy as np\n",
    "\n",
    "N, T, D, H = 2, 3, 4, 5\n",
    "x = np.linspace(-0.1, 0.3, num=N*T*D).reshape(N, T, D)\n",
    "h0 = np.linspace(-0.3, 0.1, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.2, 0.4, num=D*H).reshape(D, H)\n",
    "Wh = np.linspace(-0.4, 0.1, num=H*H).reshape(H, H)\n",
    "b = np.linspace(-0.7, 0.1, num=H)\n",
    "h, _ = rnn_forward(x, h0, Wx, Wh, b)\n",
    "expected_h = np.asarray([\n",
    "[[-0.42070749, -0.27279261, -0.11074945, 0.05740409, 0.22236251],\n",
    "[-0.39525808, -0.22554661, -0.0409454, 0.14649412, 0.32397316],\n",
    "[-0.42305111, -0.24223728, -0.04287027, 0.15997045, 0.35014525],],\n",
    "[[-0.55857474, -0.39065825, -0.19198182, 0.02378408, 0.23735671],\n",
    "[-0.27150199, -0.07088804, 0.13562939, 0.33099728, 0.50158768],\n",
    "[-0.51014825, -0.30524429, -0.06755202, 0.17806392, 0.40333043]]])\n",
    "print('h error: ', rel_error(expected_h, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_base.rnn_layers import rnn_forward\n",
    "import numpy as np\n",
    "\n",
    "x_all_shape = (3, 5, 874)\n",
    "Wx_shape = (874, 128)\n",
    "h_shape = (3, 128)\n",
    "Wh_shape = (128, 128)\n",
    "b_shape = (128,)\n",
    "x_all = np.loadtxt('./input_files/x_all.csv', delimiter=',')\n",
    "x_all = x_all.reshape(x_all_shape)\n",
    "Wx = np.loadtxt('./input_files/Wx.csv', delimiter=',')\n",
    "Wx = Wx.reshape(Wx_shape)\n",
    "h0 = np.loadtxt('./input_files/prev_h.csv', delimiter=',')\n",
    "h0 = prev_h.reshape(h_shape)\n",
    "Wh = np.loadtxt('./input_files/Wh.csv', delimiter=',')\n",
    "Wh = Wh.reshape(Wh_shape)\n",
    "b = np.loadtxt('./input_files/b.csv', delimiter=',')\n",
    "out, _ = rnn_forward(x_all, h0, Wx, Wh, b)\n",
    "np.savetxt('./output_files/rnn_forward_out.csv', out.ravel(), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  2.94540368736e-10\n",
      "dh0 error:  9.31332941507e-11\n",
      "dWx error:  1.50041578753e-10\n",
      "dWh error:  3.31189895547e-10\n",
      "db error:  3.01834222202e-11\n"
     ]
    }
   ],
   "source": [
    "from code_base.rnn_layers import rnn_forward, rnn_backward\n",
    "from code_base.gradient_check import *\n",
    "from code_base.layer_utils import *\n",
    "import numpy as np\n",
    "\n",
    "N, D, T, H = 2, 3, 10, 5\n",
    "x = np.random.randn(N, T, D)\n",
    "h0 = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, H)\n",
    "Wh = np.random.randn(H, H)\n",
    "b = np.random.randn(H)\n",
    "out, cache = rnn_forward(x, h0, Wx, Wh, b)\n",
    "dout = np.random.randn(*out.shape)\n",
    "dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n",
    "fx = lambda x: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fh0 = lambda h0: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fb = lambda b: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "dh0_num = eval_numerical_gradient_array(fh0, h0, dout)\n",
    "dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)\n",
    "dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, b, dout)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dh0 error: ', rel_error(dh0_num, dh0))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_base.rnn_layers import rnn_forward, rnn_backward\n",
    "import numpy as np\n",
    "\n",
    "x_all_shape = (3, 5, 874)\n",
    "Wx_shape = (874, 128)\n",
    "h_shape = (3, 128)\n",
    "Wh_shape = (128, 128)\n",
    "b_shape = (128,)\n",
    "dh_all_shape = (3, 5, 128)\n",
    "x_all = np.loadtxt('./input_files/x_all.csv', delimiter=',')\n",
    "x_all = x_all.reshape(x_all_shape)\n",
    "Wx = np.loadtxt('./input_files/Wx.csv', delimiter=',')\n",
    "Wx = Wx.reshape(Wx_shape)\n",
    "h0 = np.loadtxt('./input_files/prev_h.csv', delimiter=',')\n",
    "h0 = h0.reshape(h_shape)\n",
    "Wh = np.loadtxt('./input_files/Wh.csv', delimiter=',')\n",
    "Wh = Wh.reshape(Wh_shape)\n",
    "b = np.loadtxt('./input_files/b.csv', delimiter=',')\n",
    "out, cache = rnn_forward(x_all, h0, Wx, Wh, b)\n",
    "dhout = np.loadtxt('./input_files/dho_all.csv', delimiter=',')\n",
    "dhout = dhout.reshape(dh_all_shape)\n",
    "dx_all, dh0, dWx, dWh, db = rnn_backward(dhout, cache)\n",
    "np.savetxt('./output_files/rnn_backward_out_dx.csv', dx_all.ravel(), delimiter=',')\n",
    "np.savetxt('./output_files/rnn_backward_out_dh0.csv', dh0.ravel(), delimiter=',')\n",
    "np.savetxt('./output_files/rnn_backward_out_dwx.csv', dWx.ravel(), delimiter=',')\n",
    "np.savetxt('./output_files/rnn_backward_out_dwh.csv', dWh.ravel(), delimiter=',')\n",
    "np.savetxt('./output_files/rnn_backward_out_db.csv', db.ravel(), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Temporal Bi-directional Concatenation Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_base.rnn_layers import bidirectional_rnn_concatenate_forward\n",
    "from code_base.layer_utils import *\n",
    "import numpy as np\n",
    "\n",
    "N, T, H = 2, 4, 3\n",
    "h = np.linspace(-0.5, 0, num=N*T*H).reshape(N, T, H)\n",
    "hr = np.linspace(0, 0.5, num=N*T*H).reshape(N, T, H)\n",
    "mask = np.ones((N,T))\n",
    "mask[0][3] = 0 # length of s1 is 3\n",
    "mask[1][2] = mask[1][3] = 0 # length of s2 is 2\n",
    "ho, _ = bidirectional_rnn_concatenate_forward(h, hr, mask)\n",
    "expected_ho = np.array([[\n",
    "[-0.5, -0.47826087, -0.45652174, 0.13043478, 0.15217391, 0.17391304],\n",
    "[-0.43478261, -0.41304348, -0.39130435, 0.06521739, 0.08695652, 0.10869565],\n",
    "[-0.36956522, -0.34782609, -0.32608696, 0., 0.02173913, 0.04347826],\n",
    "[0., 0., 0., 0., 0., 0.]],\n",
    "[[-0.23913043, -0.2173913 , -0.19565217, 0.32608696, 0.34782609, 0.36956522],\n",
    "[-0.17391304, -0.15217391, -0.13043478, 0.26086957, 0.2826087, 0.30434783],\n",
    "[0., 0., 0., 0., 0., 0.],\n",
    "[0., 0., 0., 0., 0., 0.]]])\n",
    "print('ho error: ', rel_error(expected_ho, ho, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from code_base.rnn_layers import bidirectional_rnn_concatenate_forward\n",
    "from code_base.gradient_check import *\n",
    "import numpy as np\n",
    "\n",
    "h_shape = (3, 5, 128)\n",
    "mask_shape = (3, 5)\n",
    "h = np.loadtxt('./input_files/h_all.csv', delimiter=',')\n",
    "h = h.reshape(h_shape)\n",
    "hr = np.loadtxt('./input_files/h_all_r.csv', delimiter=',')\n",
    "hr = hr.reshape(h_shape)\n",
    "mask = np.loadtxt('./input_files/mask.csv', delimiter=',')\n",
    "mask = mask.reshape(mask_shape)\n",
    "hout, _ = bidirectional_rnn_concatenate_forward(h, hr, mask)\n",
    "np.savetxt('./output_files/bidirectional_rnn_concatenate_forward_out.csv', hout.ravel(), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Temporal Bi-directional Concatenation Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_base.rnn_layers import bidirectional_rnn_concatenate_forward, bidirectional_rnn_concatenate_backward\n",
    "from code_base.layer_utils import *\n",
    "from code_base.gradient_check import *\n",
    "import numpy as np\n",
    "\n",
    "N, T, H = 2, 4, 3\n",
    "h = np.linspace(-0.5, 0, num=N*T*H).reshape(N, T, H)\n",
    "hr = np.linspace(0, 0.5, num=N*T*H).reshape(N, T, H)\n",
    "mask = np.ones((N,T))\n",
    "mask[0][3] = 0 # length of s1 is 3\n",
    "mask[1][2] = mask[1][3] = 0 # length of s2 is 2\n",
    "ho, cache = bidirectional_rnn_concatenate_forward(h, hr, mask)\n",
    "dho = np.linspace(0., 0.5, num=N*T*2*H).reshape(N, T, 2*H)\n",
    "dh, dhr = bidirectional_rnn_concatenate_backward(dho, cache)\n",
    "fh = lambda h: bidirectional_rnn_concatenate_forward(h, hr, mask)[0]\n",
    "fhr = lambda hr: bidirectional_rnn_concatenate_forward(h, hr, mask)[0]\n",
    "dh_num = eval_numerical_gradient_array(fh, h, dho)\n",
    "dhr_num = eval_numerical_gradient_array(fhr, hr, dho)\n",
    "print('dh error: ', rel_error(dh_num, dh, mask))\n",
    "print('dhr error: ', rel_error(dhr_num, dhr, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from code_base.rnn_layers import bidirectional_rnn_concatenate_forward, bidirectional_rnn_concatenate_backward\n",
    "import numpy as np\n",
    "\n",
    "h_shape = (3, 5, 128)\n",
    "mask_shape = (3, 5)\n",
    "h = np.loadtxt('./input_files/h_all.csv', delimiter=',')\n",
    "h = h.reshape(h_shape)\n",
    "hr = np.loadtxt('./input_files/h_all_r.csv', delimiter=',')\n",
    "hr = hr.reshape(h_shape)\n",
    "mask = np.loadtxt('./input_files/mask.csv', delimiter=',')\n",
    "mask = mask.reshape(mask_shape)\n",
    "hout, cache = bidirectional_rnn_concatenate_forward(h, hr, mask)\n",
    "dhout = np.loadtxt('./input_files/dhc_all.csv', delimiter=',')\n",
    "dhout = dhout.reshape(3, 5, 256)\n",
    "dh, dhr = bidirectional_rnn_concatenate_backward(dhout, cache)\n",
    "np.savetxt('./output_files/bidirectional_rnn_concatenate_backward_out_h.csv', dh.ravel(), delimiter=',')\n",
    "np.savetxt('./output_files/bidirectional_rnn_concatenate_backward_out_hr.csv', dhr.ravel(), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for Sentiment Analysis - Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Model\n",
      "N: 2, T: 4, H: 6, V: 10\n",
      "h.shape: (2, 4, 6)\n",
      "W_a.shape: (6, 5)\n",
      "ta_out.shape: (2, 4, 5)\n",
      "A: 5\n",
      "av_out.shape: (2, 5)\n",
      "mask.shape: (2, 4)\n",
      "[[-0.39778907  0.41714814  1.23208534  2.04702255  2.86195976]\n",
      " [ 0.54151403  1.36386563  2.18621723  3.00856883  3.83092043]]\n",
      "[[ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  0.]]\n",
      "W_fc.shape: (5, 2)\n",
      "a_out.shape: (2, 2)\n",
      "loss:  2.99619226823\n",
      "expected loss:  2.99619226823\n",
      "difference:  1.38733469157e-12\n"
     ]
    }
   ],
   "source": [
    "from code_base.classifiers.rnn import *\n",
    "# If you do brnn, please import from code_base.classifiers.brnn instead\n",
    "import numpy as np\n",
    "\n",
    "N, H, A, O = 2, 6, 5, 2\n",
    "word_to_idx = { 'awesome': 0, 'reading':1, 'pretty': 2, 'dog': 3, 'movie': 4,\n",
    "                'liked': 5, 'most': 6, 'admired': 7, 'bad': 8, 'fucking': 9}\n",
    "V = len(word_to_idx)\n",
    "T = 4\n",
    "\n",
    "model = SentimentAnalysisRNN(word_to_idx,\n",
    "    hidden_dim=H,\n",
    "    fc_dim=A,\n",
    "    output_dim=O,\n",
    "    cell_type='rnn',\n",
    "    dtype=np.float64)\n",
    "# Set all model parameters to fixed values\n",
    "for k, v in model.params.items():\n",
    "    model.params[k] = np.linspace(-1.4, 1.3, num=v.size).reshape(*v.shape)\n",
    "labels = np.array([1, 0], dtype=np.int32)\n",
    "wordvecs = np.zeros((N, T, V))\n",
    "wordvecs[0, 0, 0] = wordvecs[0, 1, 5] = wordvecs[0, 2, 2] = wordvecs[0, 3, 7] = 1\n",
    "wordvecs[1, 0, 4] = wordvecs[1, 1, 8] = wordvecs[1, 2, 5] = 1\n",
    "mask = np.ones((N, T))\n",
    "mask[1, 3] = 0\n",
    "loss, grads = model.loss(wordvecs, labels, mask)\n",
    "expected_loss = 2.99619226823\n",
    "# For brnn, the expected_loss should be 2.9577205234\n",
    "print('loss: ', loss)\n",
    "print('expected loss: ', expected_loss)\n",
    "print('difference: ', abs(loss - expected_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for Sentiment Analysis - Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Model\n",
      "W_a relative error: 2.470576e-09\n",
      "W_fc relative error: 2.570463e-10\n",
      "Wh relative error: 1.117105e-07\n",
      "Wx relative error: 3.993306e-08\n",
      "b relative error: 9.065656e-09\n",
      "b_a relative error: 1.536972e-09\n",
      "b_fc relative error: 4.681212e-12\n"
     ]
    }
   ],
   "source": [
    "from code_base.classifiers.rnn import *\n",
    "# If you do brnn, please import from code_base.classifiers.brnn instead\n",
    "from code_base.gradient_check import *\n",
    "from code_base.layer_utils import rel_error\n",
    "import numpy as np\n",
    "\n",
    "N, T, H, A, O = 2, 4, 6, 5, 2\n",
    "word_to_idx = {'awesome': 0, 'reading':1, 'pretty': 2, 'dog': 3, 'movie': 4,\n",
    "                'liked': 5, 'most': 6, 'admired': 7, 'bad': 8, 'fucking': 9}\n",
    "V = len(word_to_idx)\n",
    "labels = np.array([1, 0], dtype=np.int32)\n",
    "wordvecs = np.zeros((N, T, V))\n",
    "wordvecs[0, 0, 0] = wordvecs[0, 1, 5] = wordvecs[0, 2, 2] = wordvecs[0, 3, 7] = 1\n",
    "wordvecs[1, 0, 4] = wordvecs[1, 1, 8] = wordvecs[1, 2, 5] = 1\n",
    "mask = np.ones((N, T))\n",
    "mask[1, 3] = 0\n",
    "model = SentimentAnalysisRNN(word_to_idx,\n",
    "    hidden_dim=H,\n",
    "    fc_dim=A,\n",
    "    output_dim=O,\n",
    "    cell_type='rnn',\n",
    "    dtype=np.float64,\n",
    ")\n",
    "loss, grads = model.loss(wordvecs, labels, mask)\n",
    "for param_name in sorted(grads):\n",
    "    f = lambda _: model.loss(wordvecs, labels, mask)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, model.params[param_name],\n",
    "verbose=False, h=1e-6)\n",
    "    e = rel_error(param_grad_num, grads[param_name])\n",
    "    print('%s relative error: %e' % (param_name, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Inference on Small Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jiaxun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "(Iteration 1 / 100) loss: 0.315483\n",
      "(Iteration 11 / 100) loss: 0.277323\n",
      "(Iteration 21 / 100) loss: 0.243838\n",
      "(Iteration 31 / 100) loss: 0.212798\n",
      "(Iteration 41 / 100) loss: 0.183810\n",
      "(Iteration 51 / 100) loss: 0.157254\n",
      "(Iteration 61 / 100) loss: 0.133639\n",
      "(Iteration 71 / 100) loss: 0.113249\n",
      "(Iteration 81 / 100) loss: 0.096055\n",
      "(Iteration 91 / 100) loss: 0.081789\n",
      "preds.shape: (100, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRAqksBARkQIpLEREpEAKCxERKdD/AwyV93CISWIbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f38b278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from code_base.sentiment_analysis_solver import *\n",
    "from code_base.classifiers.rnn import *\n",
    "# If you do brnn, please import from code_base.classifiers.brnn instead\n",
    "from code_base.data_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "download_corpus()\n",
    "small_data = load_data('code_base/datasets/train.csv', sample=True)\n",
    "small_rnn_model = SentimentAnalysisRNN(\n",
    "    cell_type='rnn',\n",
    "    word_to_idx=load_dictionary('code_base/datasets/dictionary.csv')\n",
    ")\n",
    "small_rnn_solver = SentimentAnalysisSolver(small_rnn_model,\n",
    "    small_data,\n",
    "    update_rule='sgd',\n",
    "    num_epochs=100,\n",
    "    batch_size=100,\n",
    "    optim_config={\n",
    "        'learning_rate': 5e-3,\n",
    "    },\n",
    "    lr_decay=1.0,\n",
    "    verbose=True,\n",
    "    print_every=10,\n",
    ")\n",
    "small_rnn_solver.train()\n",
    "\n",
    "# we will use the same batch of training data for inference\n",
    "# this is just to let you know the procedure of inference\n",
    "preds = small_rnn_solver.test(split='train')\n",
    "np.savetxt('./output_files/rnn_prediction_prob.csv', preds.ravel(), delimiter=',')\n",
    "# If you do brnn, please save result to ./output_files/brnn_prediction_prob.csv\n",
    "\n",
    "# Plot the training losses\n",
    "plt.plot(small_rnn_solver.loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
